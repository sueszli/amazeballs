{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Only for colab bc otherwise all the drive mapping etc gets super annoying.",
      "metadata": {
        "id": "Qj48ei4e-lxO"
      }
    },
    {
      "cell_type": "code",
      "source": "%%writefile requirements.txt\n# TODO fix requirements since pyzmq upgrade breaks colab\n#\n# This file is autogenerated by pip-compile with Python 3.11\n# by the following command:\n#\n#    pip-compile --output-file=requirements.txt requirements.in\n#\naccelerate==1.1.1\n    # via\n    #   -r requirements.in\n    #   sentence-transformers\naiohappyeyeballs==2.4.3\n    # via\n    #   -r requirements.in\n    #   aiohttp\naiohttp==3.11.7\n    # via\n    #   -r requirements.in\n    #   datasets\n    #   fsspec\naiosignal==1.3.1\n    # via\n    #   -r requirements.in\n    #   aiohttp\nannotated-types==0.7.0\n    # via\n    #   -r requirements.in\n    #   pydantic\nappnope==0.1.4\n    # via\n    #   -r requirements.in\n    #   ipykernel\nasttokens==2.4.1\n    # via\n    #   -r requirements.in\n    #   stack-data\nattrs==24.2.0\n    # via\n    #   -r requirements.in\n    #   aiohttp\nautocuda==0.16\n    # via\n    #   -r requirements.in\n    #   pyabsa\nautoflake==2.3.1\n    # via -r requirements.in\nbertopic==0.16.4\n    # via -r requirements.in\nblis==0.7.11\n    # via\n    #   -r requirements.in\n    #   thinc\nboostaug==2.3.5\n    # via\n    #   -r requirements.in\n    #   pyabsa\ncatalogue==2.0.10\n    # via\n    #   -r requirements.in\n    #   spacy\n    #   srsly\n    #   thinc\ncertifi==2024.8.30\n    # via\n    #   -r requirements.in\n    #   requests\ncharset-normalizer==3.4.0\n    # via\n    #   -r requirements.in\n    #   requests\nclick==8.1.7\n    # via\n    #   -r requirements.in\n    #   metric-visualizer\n    #   nltk\n    #   typer\n    #   yake\ncloudpathlib==0.20.0\n    # via\n    #   -r requirements.in\n    #   weasel\ncomm==0.2.2\n    # via\n    #   -r requirements.in\n    #   ipykernel\n    #   ipywidgets\nconfection==0.1.5\n    # via\n    #   -r requirements.in\n    #   thinc\n    #   weasel\ncontourpy==1.3.1\n    # via\n    #   -r requirements.in\n    #   matplotlib\ncycler==0.12.1\n    # via\n    #   -r requirements.in\n    #   matplotlib\ncymem==2.0.10\n    # via\n    #   -r requirements.in\n    #   preshed\n    #   spacy\n    #   thinc\ndatasets==3.1.0\n    # via\n    #   -r requirements.in\n    #   evaluate\n    #   sentence-transformers\n    #   setfit\ndebugpy==1.8.9\n    # via\n    #   -r requirements.in\n    #   ipykernel\ndecorator==5.1.1\n    # via\n    #   -r requirements.in\n    #   ipython\ndill==0.3.8\n    # via\n    #   -r requirements.in\n    #   datasets\n    #   evaluate\n    #   multiprocess\nen-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889\n    # via -r requirements.in\net-xmlfile==2.0.0\n    # via\n    #   -r requirements.in\n    #   openpyxl\nevaluate==0.4.3\n    # via\n    #   -r requirements.in\n    #   setfit\nexecuting==2.1.0\n    # via\n    #   -r requirements.in\n    #   stack-data\nfilelock==3.16.1\n    # via\n    #   -r requirements.in\n    #   datasets\n    #   huggingface-hub\n    #   torch\n    #   transformers\nfindfile==2.0.1\n    # via\n    #   -r requirements.in\n    #   metric-visualizer\n    #   pyabsa\nfonttools==4.55.0\n    # via\n    #   -r requirements.in\n    #   matplotlib\nfrozenlist==1.5.0\n    # via\n    #   -r requirements.in\n    #   aiohttp\n    #   aiosignal\nfsspec[http]==2024.9.0\n    # via\n    #   -r requirements.in\n    #   datasets\n    #   evaluate\n    #   huggingface-hub\n    #   torch\ngitdb==4.0.11\n    # via\n    #   -r requirements.in\n    #   gitpython\ngitpython==3.1.43\n    # via\n    #   -r requirements.in\n    #   pyabsa\nhdbscan==0.8.40\n    # via\n    #   -r requirements.in\n    #   bertopic\nhuggingface-hub==0.26.2\n    # via\n    #   -r requirements.in\n    #   accelerate\n    #   datasets\n    #   evaluate\n    #   sentence-transformers\n    #   setfit\n    #   tokenizers\n    #   transformers\nidna==3.10\n    # via\n    #   -r requirements.in\n    #   requests\n    #   yarl\nipykernel==6.29.5\n    # via -r requirements.in\nipython==8.29.0\n    # via\n    #   -r requirements.in\n    #   ipykernel\n    #   ipywidgets\nipywidgets==8.1.5\n    # via -r requirements.in\nisort==5.13.2\n    # via -r requirements.in\njedi==0.19.2\n    # via\n    #   -r requirements.in\n    #   ipython\njellyfish==1.1.0\n    # via\n    #   -r requirements.in\n    #   yake\njinja2==3.1.4\n    # via\n    #   -r requirements.in\n    #   spacy\n    #   torch\njoblib==1.4.2\n    # via\n    #   -r requirements.in\n    #   hdbscan\n    #   nltk\n    #   pynndescent\n    #   scikit-learn\njupyter-client==8.6.3\n    # via\n    #   -r requirements.in\n    #   ipykernel\njupyter-core==5.7.2\n    # via\n    #   -r requirements.in\n    #   ipykernel\n    #   jupyter-client\njupyterlab-widgets==3.0.13\n    # via\n    #   -r requirements.in\n    #   ipywidgets\nkiwisolver==1.4.7\n    # via\n    #   -r requirements.in\n    #   matplotlib\nlangcodes==3.5.0\n    # via\n    #   -r requirements.in\n    #   spacy\nlangdetect==1.0.9\n    # via -r requirements.in\nlanguage-data==1.3.0\n    # via\n    #   -r requirements.in\n    #   langcodes\nllvmlite==0.43.0\n    # via\n    #   -r requirements.in\n    #   numba\n    #   pynndescent\nmarisa-trie==1.2.1\n    # via\n    #   -r requirements.in\n    #   language-data\nmarkdown-it-py==3.0.0\n    # via\n    #   -r requirements.in\n    #   rich\nmarkupsafe==3.0.2\n    # via\n    #   -r requirements.in\n    #   jinja2\nmatplotlib==3.9.2\n    # via\n    #   -r requirements.in\n    #   metric-visualizer\n    #   plotnine\n    #   tikzplotlib\nmatplotlib-inline==0.1.7\n    # via\n    #   -r requirements.in\n    #   ipykernel\n    #   ipython\nmdurl==0.1.2\n    # via\n    #   -r requirements.in\n    #   markdown-it-py\nmetric-visualizer==0.9.13.post1\n    # via\n    #   -r requirements.in\n    #   pyabsa\nmizani==0.13.0\n    # via\n    #   -r requirements.in\n    #   plotnine\nmpmath==1.3.0\n    # via\n    #   -r requirements.in\n    #   sympy\nmultidict==6.1.0\n    # via\n    #   -r requirements.in\n    #   aiohttp\n    #   yarl\nmultiprocess==0.70.16\n    # via\n    #   -r requirements.in\n    #   datasets\n    #   evaluate\nmurmurhash==1.0.11\n    # via\n    #   -r requirements.in\n    #   preshed\n    #   spacy\n    #   thinc\nnatsort==8.4.0\n    # via\n    #   -r requirements.in\n    #   metric-visualizer\nnest-asyncio==1.6.0\n    # via\n    #   -r requirements.in\n    #   ipykernel\nnetworkx==3.4.2\n    # via\n    #   -r requirements.in\n    #   pyabsa\n    #   torch\n    #   yake\nnltk==3.9.1\n    # via\n    #   -r requirements.in\n    #   textblob\nnumba==0.60.0\n    # via\n    #   -r requirements.in\n    #   pynndescent\n    #   umap-learn\nnumpy==1.26.4\n    # via\n    #   -r requirements.in\n    #   accelerate\n    #   bertopic\n    #   blis\n    #   contourpy\n    #   datasets\n    #   evaluate\n    #   hdbscan\n    #   matplotlib\n    #   metric-visualizer\n    #   mizani\n    #   numba\n    #   pandas\n    #   patsy\n    #   plotnine\n    #   scikit-learn\n    #   scipy\n    #   seqeval\n    #   spacy\n    #   statsmodels\n    #   thinc\n    #   tikzplotlib\n    #   torchvision\n    #   transformers\n    #   umap-learn\n    #   yake\nopenpyxl==3.1.5\n    # via\n    #   -r requirements.in\n    #   metric-visualizer\npackaging==24.2\n    # via\n    #   -r requirements.in\n    #   accelerate\n    #   datasets\n    #   evaluate\n    #   huggingface-hub\n    #   ipykernel\n    #   matplotlib\n    #   plotly\n    #   setfit\n    #   spacy\n    #   statsmodels\n    #   thinc\n    #   transformers\n    #   weasel\npandas==2.2.3\n    # via\n    #   -r requirements.in\n    #   bertopic\n    #   datasets\n    #   evaluate\n    #   metric-visualizer\n    #   mizani\n    #   plotnine\n    #   pyabsa\n    #   statsmodels\nparso==0.8.4\n    # via\n    #   -r requirements.in\n    #   jedi\npatsy==1.0.1\n    # via\n    #   -r requirements.in\n    #   statsmodels\npexpect==4.9.0\n    # via\n    #   -r requirements.in\n    #   ipython\npillow==11.0.0\n    # via\n    #   -r requirements.in\n    #   matplotlib\n    #   sentence-transformers\n    #   tikzplotlib\n    #   torchvision\nplatformdirs==4.3.6\n    # via\n    #   -r requirements.in\n    #   jupyter-core\nplotly==5.24.1\n    # via\n    #   -r requirements.in\n    #   bertopic\nplotnine==0.14.3\n    # via -r requirements.in\npreshed==3.0.9\n    # via\n    #   -r requirements.in\n    #   spacy\n    #   thinc\nprompt-toolkit==3.0.48\n    # via\n    #   -r requirements.in\n    #   ipython\npropcache==0.2.0\n    # via\n    #   -r requirements.in\n    #   aiohttp\n    #   yarl\nprotobuf==3.20.3\n    # via\n    #   -r requirements.in\n    #   pyabsa\npsutil==6.1.0\n    # via\n    #   -r requirements.in\n    #   accelerate\n    #   ipykernel\nptyprocess==0.7.0\n    # via\n    #   -r requirements.in\n    #   pexpect\npure-eval==0.2.3\n    # via\n    #   -r requirements.in\n    #   stack-data\npyabsa==2.4.1.post1\n    # via\n    #   -r requirements.in\n    #   boostaug\npyarrow==18.1.0\n    # via\n    #   -r requirements.in\n    #   datasets\npydantic==2.10.2\n    # via\n    #   -r requirements.in\n    #   confection\n    #   spacy\n    #   thinc\n    #   weasel\npydantic-core==2.27.1\n    # via\n    #   -r requirements.in\n    #   pydantic\npyflakes==3.2.0\n    # via\n    #   -r requirements.in\n    #   autoflake\npygments==2.18.0\n    # via\n    #   -r requirements.in\n    #   ipython\n    #   rich\npynndescent==0.5.13\n    # via\n    #   -r requirements.in\n    #   umap-learn\npyparsing==3.2.0\n    # via\n    #   -r requirements.in\n    #   matplotlib\npython-dateutil==2.9.0.post0\n    # via\n    #   -r requirements.in\n    #   jupyter-client\n    #   matplotlib\n    #   pandas\npytorch-warmup==0.2.0\n    # via\n    #   -r requirements.in\n    #   pyabsa\npytz==2024.2\n    # via\n    #   -r requirements.in\n    #   pandas\npyyaml==6.0.2\n    # via\n    #   -r requirements.in\n    #   accelerate\n    #   datasets\n    #   huggingface-hub\n    #   transformers\nregex==2024.11.6\n    # via\n    #   -r requirements.in\n    #   nltk\n    #   segtok\n    #   tiktoken\n    #   transformers\nrequests==2.32.3\n    # via\n    #   -r requirements.in\n    #   datasets\n    #   evaluate\n    #   huggingface-hub\n    #   spacy\n    #   tiktoken\n    #   transformers\n    #   update-checker\n    #   weasel\nrich==13.9.4\n    # via\n    #   -r requirements.in\n    #   typer\nruff==0.8.0\n    # via -r requirements.in\nsafetensors==0.4.5\n    # via\n    #   -r requirements.in\n    #   accelerate\n    #   transformers\nscikit-learn==1.5.2\n    # via\n    #   -r requirements.in\n    #   bertopic\n    #   hdbscan\n    #   metric-visualizer\n    #   pynndescent\n    #   sentence-transformers\n    #   seqeval\n    #   setfit\n    #   umap-learn\nscipy==1.10.1\n    # via\n    #   -r requirements.in\n    #   hdbscan\n    #   metric-visualizer\n    #   mizani\n    #   plotnine\n    #   pynndescent\n    #   scikit-learn\n    #   sentence-transformers\n    #   statsmodels\n    #   summa\n    #   umap-learn\nsegtok==1.5.11\n    # via\n    #   -r requirements.in\n    #   yake\nsentence-transformers[train]==3.3.1\n    # via\n    #   -r requirements.in\n    #   bertopic\n    #   setfit\nsentencepiece==0.2.0\n    # via\n    #   -r requirements.in\n    #   pyabsa\nseqeval==1.2.2\n    # via\n    #   -r requirements.in\n    #   pyabsa\nsetfit==1.1.0\n    # via -r requirements.in\nshellingham==1.5.4\n    # via\n    #   -r requirements.in\n    #   typer\nsix==1.16.0\n    # via\n    #   -r requirements.in\n    #   asttokens\n    #   langdetect\n    #   python-dateutil\nsmart-open==7.0.5\n    # via\n    #   -r requirements.in\n    #   weasel\nsmmap==5.0.1\n    # via\n    #   -r requirements.in\n    #   gitdb\nspacy==3.7.5\n    # via\n    #   -r requirements.in\n    #   en-core-web-sm\n    #   pyabsa\nspacy-legacy==3.0.12\n    # via\n    #   -r requirements.in\n    #   spacy\nspacy-loggers==1.0.5\n    # via\n    #   -r requirements.in\n    #   spacy\nsrsly==2.4.8\n    # via\n    #   -r requirements.in\n    #   confection\n    #   spacy\n    #   thinc\n    #   weasel\nstack-data==0.6.3\n    # via\n    #   -r requirements.in\n    #   ipython\nstatsmodels==0.14.4\n    # via\n    #   -r requirements.in\n    #   plotnine\nsumma==1.2.0\n    # via -r requirements.in\nsympy==1.13.1\n    # via\n    #   -r requirements.in\n    #   torch\ntabulate==0.9.0\n    # via\n    #   -r requirements.in\n    #   metric-visualizer\n    #   yake\ntenacity==9.0.0\n    # via\n    #   -r requirements.in\n    #   plotly\ntermcolor==2.5.0\n    # via\n    #   -r requirements.in\n    #   findfile\n    #   pyabsa\ntextblob==0.18.0.post0\n    # via -r requirements.in\nthinc==8.2.5\n    # via\n    #   -r requirements.in\n    #   spacy\nthreadpoolctl==3.5.0\n    # via\n    #   -r requirements.in\n    #   scikit-learn\ntiktoken==0.8.0\n    # via -r requirements.in\ntikzplotlib==0.10.1\n    # via\n    #   -r requirements.in\n    #   metric-visualizer\ntokenizers==0.20.3\n    # via\n    #   -r requirements.in\n    #   transformers\ntorch==2.5.1\n    # via\n    #   -r requirements.in\n    #   accelerate\n    #   pyabsa\n    #   pytorch-warmup\n    #   sentence-transformers\n    #   torchvision\ntorchvision==0.20.1\n    # via -r requirements.in\ntornado==6.4.2\n    # via\n    #   -r requirements.in\n    #   ipykernel\n    #   jupyter-client\ntqdm==4.67.1\n    # via\n    #   -r requirements.in\n    #   bertopic\n    #   datasets\n    #   evaluate\n    #   huggingface-hub\n    #   nltk\n    #   pyabsa\n    #   sentence-transformers\n    #   spacy\n    #   transformers\n    #   umap-learn\ntraitlets==5.14.3\n    # via\n    #   -r requirements.in\n    #   comm\n    #   ipykernel\n    #   ipython\n    #   ipywidgets\n    #   jupyter-client\n    #   jupyter-core\n    #   matplotlib-inline\ntransformers==4.46.3\n    # via\n    #   -r requirements.in\n    #   pyabsa\n    #   sentence-transformers\n    #   setfit\ntyper==0.13.1\n    # via\n    #   -r requirements.in\n    #   spacy\n    #   weasel\ntyping-extensions==4.12.2\n    # via\n    #   -r requirements.in\n    #   huggingface-hub\n    #   ipython\n    #   pyabsa\n    #   pydantic\n    #   pydantic-core\n    #   torch\n    #   typer\ntzdata==2024.2\n    # via\n    #   -r requirements.in\n    #   pandas\numap-learn==0.5.7\n    # via\n    #   -r requirements.in\n    #   bertopic\nupdate-checker==0.18.0\n    # via\n    #   -r requirements.in\n    #   metric-visualizer\n    #   pyabsa\nurllib3==2.2.3\n    # via\n    #   -r requirements.in\n    #   requests\nwasabi==1.1.3\n    # via\n    #   -r requirements.in\n    #   spacy\n    #   thinc\n    #   weasel\nwcwidth==0.2.13\n    # via\n    #   -r requirements.in\n    #   prompt-toolkit\nweasel==0.4.1\n    # via\n    #   -r requirements.in\n    #   spacy\nwebcolors==24.11.1\n    # via\n    #   -r requirements.in\n    #   tikzplotlib\nwidgetsnbextension==4.0.13\n    # via\n    #   -r requirements.in\n    #   ipywidgets\nwrapt==1.17.0\n    # via\n    #   -r requirements.in\n    #   smart-open\nxlsxwriter==3.2.0\n    # via\n    #   -r requirements.in\n    #   metric-visualizer\nxxhash==3.5.0\n    # via\n    #   -r requirements.in\n    #   datasets\n    #   evaluate\nyake==0.4.8\n    # via -r requirements.in\nyarl==1.18.0\n    # via\n    #   -r requirements.in\n    #   aiohttp\n\n# The following packages are considered to be unsafe in a requirements file:\n# setuptools\n\n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raP4dVn1VnnO",
        "outputId": "d580698f-d923-4eb2-83fe-eaef1df817da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting requirements.txt\n"
        }
      ],
      "execution_count": 59
    },
    {
      "cell_type": "code",
      "source": "!pip install -r requirements.txt\nfrom IPython.display import clear_output\nclear_output()",
      "metadata": {
        "id": "_KqZQ4S_VxD0"
      },
      "outputs": [],
      "execution_count": 60
    },
    {
      "cell_type": "code",
      "source": "sample_size = 1000\nseed = 42",
      "metadata": {
        "id": "PQTpFO2F_X04"
      },
      "outputs": [],
      "execution_count": 61
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "id": "zyRx38_pA8MO"
      }
    },
    {
      "cell_type": "code",
      "source": "import functools\nimport gc\nimport os\nimport random\nimport time\nfrom contextlib import contextmanager\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm",
      "metadata": {
        "id": "aFp6SEXbCBlF"
      },
      "outputs": [],
      "execution_count": 62
    },
    {
      "cell_type": "code",
      "source": "def get_current_dir() -> Path:\n    try:\n        return Path(__file__).parent.absolute()\n    except NameError:\n        return Path(os.getcwd())\n\n\nif seed == -1:\n    seed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n    # perf\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"  # (range: 16-512)\n\n\n\ndata_path = get_current_dir() / \"data\"\ndataset_path = get_current_dir() / \"datasets\"\nweights_path = get_current_dir() / \"weights\"\n\nos.makedirs(data_path, exist_ok=True)\nos.makedirs(dataset_path, exist_ok=True)\nos.makedirs(weights_path, exist_ok=True)",
      "metadata": {
        "id": "rX799tFW_XRV"
      },
      "outputs": [],
      "execution_count": 63
    },
    {
      "cell_type": "markdown",
      "source": "# Question definition\n We chose task 21 from the list. Since these questions already cover a wide range of topics we did not modify them and took them as they were:\n\n\n\n1.   (RQ1) Are reviews for some categories of product on Amazon overall more positive than for other categories?\n2.   (RQ2) Are reviews more subjective for some classes of products than for others?\n3. (RQ3) Which aspects of different classes of products are the most important in the reviews?\n4. (RQ4) Can one predict the star rating from the review text?",
      "metadata": {
        "id": "nQZfCvghXjuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Choose Dataset\n\nNow we went to look for data to answer all our questions. We found the Amazon Reviews’23 dataset which is a standarrd dataset in the RecSys and NLP community. It consists of 571.54M reviews with data from May 1996 to September 2023. It is also easily accessible through the Hugging Face library which made our lives much easier in data loading and preprocessing.  ",
      "metadata": {
        "id": "Xli7NI-GV3mF"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We are caching dataset locally so we don't have to reload them all the time.\nUsing datasets to download the set.\nAfter that we sample data with our given sample size and save it to cache it if needed",
      "metadata": {
        "id": "rxXgtSKz71Kf"
      }
    },
    {
      "cell_type": "code",
      "source": "import datasets\ndef get_all_categories():\n\n\n    data = datasets.load_dataset(\"text\",\n                                 data_files=\"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/raw/main/all_categories.txt\",\n                                 streaming=False, cache_dir=dataset_path, trust_remote_code=True)\n    return data[\"train\"].to_dict()[\"text\"]\n\ndef get_category_data(category, sample_size, seed):\n\n    # cache hit\n    cachepath = dataset_path / f\"cache_{category}_{sample_size}.csv\"\n    if cachepath.exists():\n        data = pd.read_csv(cachepath)\n        return data\n\n    # cache miss\n    data = datasets.load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_review_{category}\", streaming=True,\n                                 cache_dir=dataset_path, trust_remote_code=True)\n    sampled_data = []\n    for entry in tqdm(data[\"full\"].shuffle(seed=seed).take(sample_size), total=sample_size, desc=f\"sampling {category}\",\n                      ncols=100):\n        sampled_data.append(entry)\n    data = pd.DataFrame(sampled_data)\n    data[\"category\"] = category  # add category column\n    data.to_csv(cachepath, index=False)\n    return data\n",
      "metadata": {
        "id": "Wx13zsKW7ywF"
      },
      "outputs": [],
      "execution_count": 64
    },
    {
      "cell_type": "markdown",
      "source": "Again Caching, here we load the different sampled categories (sample-size rows per category) and push them to a complete csv",
      "metadata": {
        "id": "a5Le6Slc8fAU"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "BkDB2kJDdtr3"
      },
      "outputs": [],
      "execution_count": 64
    },
    {
      "cell_type": "code",
      "source": "\n# cache hit\ncachepath = dataset_path / f\"cache_all_{sample_size}.csv\"\nif cachepath.exists():\n    data = pd.read_csv(cachepath)\n    print(f\"total data size: {data.memory_usage(deep=True).sum() / 1e9:.2f} gb\")\n\nelse:\n  # cache miss\n  data = pd.DataFrame()\n  categories = get_all_categories()\n  for category in tqdm(categories, desc=\"loading all data\", ncols=100):\n      category_data = get_category_data(category, sample_size,seed)\n      data = pd.concat([data, category_data], ignore_index=True)\n      tqdm.write(\n          f\"loaded {category} - category size: {category_data.memory_usage(deep=True).sum() / 1e9:.2f} gb, total size: {data.memory_usage(deep=True).sum() / 1e9:.2f} gb\")\n  data.to_csv(cachepath, index=False)\n  print(f\"total data size: {data.memory_usage(deep=True).sum() / 1e9:.2f} gb\")\ndf = data",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxQP9oHG7WuA",
        "outputId": "747eeb73-a0e9-45f0-d053-3e732e254364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "total data size: 0.03 gb\n"
        }
      ],
      "execution_count": 65
    },
    {
      "cell_type": "code",
      "source": "df.head()",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "FsscpYaf-cLh",
        "outputId": "36defdb3-946c-4028-a249-dcd4a27e85e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   rating                                              title  \\\n",
              "0     4.0                                               Soft   \n",
              "1     4.0  TRY THIS! test on FACE for match not hands, ev...   \n",
              "2     5.0                                  Excellent product   \n",
              "3     5.0                                         Five Stars   \n",
              "4     5.0                                Awesome foundation!   \n",
              "\n",
              "                                                text  \\\n",
              "0  I wear headbands like this in the evening when...   \n",
              "1  I first tried it on the back of my hand and wa...   \n",
              "2                   This is so very good for my hair   \n",
              "3          My daughter loves the foot pedicure unit.   \n",
              "4  I am so glad they made a product like this bec...   \n",
              "\n",
              "                                              images        asin parent_asin  \\\n",
              "0                                                 []  B082NKQ4ZT  B082NKQ4ZT   \n",
              "1  [{'small_image_url': 'https://images-na.ssl-im...  B08GSKRQKD  B08GSKRQKD   \n",
              "2                                                 []  B01DD1NOZU  B01DD1NOZU   \n",
              "3                                                 []  B01H1RGQGQ  B01H1RGQGQ   \n",
              "4  [{'small_image_url': 'https://images-na.ssl-im...  B07V9V5R48  B07V9V5R48   \n",
              "\n",
              "                        user_id      timestamp  helpful_vote  \\\n",
              "0  AHV6QCNBJNSGLATP56JAWJ3C4G2A  1583932042329             0   \n",
              "1  AF2BLE54TEMGZ546U763ZHZRXC4A  1612277896403             0   \n",
              "2  AHU2GG5RF6YAEWUFNLH3QH5RHDNQ  1637952741170             0   \n",
              "3  AGTR4A6CYH6AGEIYCAPYPQZERZLQ  1486999572000             0   \n",
              "4  AFV22L7AEKI2LW6HMLRLUKNYVBGQ  1568738284832             7   \n",
              "\n",
              "   verified_purchase    category  \n",
              "0              False  All_Beauty  \n",
              "1              False  All_Beauty  \n",
              "2               True  All_Beauty  \n",
              "3               True  All_Beauty  \n",
              "4               True  All_Beauty  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bb984bc-2d56-4f8a-8d1d-77afdaeae58d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>images</th>\n",
              "      <th>asin</th>\n",
              "      <th>parent_asin</th>\n",
              "      <th>user_id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>helpful_vote</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>Soft</td>\n",
              "      <td>I wear headbands like this in the evening when...</td>\n",
              "      <td>[]</td>\n",
              "      <td>B082NKQ4ZT</td>\n",
              "      <td>B082NKQ4ZT</td>\n",
              "      <td>AHV6QCNBJNSGLATP56JAWJ3C4G2A</td>\n",
              "      <td>1583932042329</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>All_Beauty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.0</td>\n",
              "      <td>TRY THIS! test on FACE for match not hands, ev...</td>\n",
              "      <td>I first tried it on the back of my hand and wa...</td>\n",
              "      <td>[{'small_image_url': 'https://images-na.ssl-im...</td>\n",
              "      <td>B08GSKRQKD</td>\n",
              "      <td>B08GSKRQKD</td>\n",
              "      <td>AF2BLE54TEMGZ546U763ZHZRXC4A</td>\n",
              "      <td>1612277896403</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>All_Beauty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Excellent product</td>\n",
              "      <td>This is so very good for my hair</td>\n",
              "      <td>[]</td>\n",
              "      <td>B01DD1NOZU</td>\n",
              "      <td>B01DD1NOZU</td>\n",
              "      <td>AHU2GG5RF6YAEWUFNLH3QH5RHDNQ</td>\n",
              "      <td>1637952741170</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>All_Beauty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>My daughter loves the foot pedicure unit.</td>\n",
              "      <td>[]</td>\n",
              "      <td>B01H1RGQGQ</td>\n",
              "      <td>B01H1RGQGQ</td>\n",
              "      <td>AGTR4A6CYH6AGEIYCAPYPQZERZLQ</td>\n",
              "      <td>1486999572000</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>All_Beauty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Awesome foundation!</td>\n",
              "      <td>I am so glad they made a product like this bec...</td>\n",
              "      <td>[{'small_image_url': 'https://images-na.ssl-im...</td>\n",
              "      <td>B07V9V5R48</td>\n",
              "      <td>B07V9V5R48</td>\n",
              "      <td>AFV22L7AEKI2LW6HMLRLUKNYVBGQ</td>\n",
              "      <td>1568738284832</td>\n",
              "      <td>7</td>\n",
              "      <td>True</td>\n",
              "      <td>All_Beauty</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bb984bc-2d56-4f8a-8d1d-77afdaeae58d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2bb984bc-2d56-4f8a-8d1d-77afdaeae58d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2bb984bc-2d56-4f8a-8d1d-77afdaeae58d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-90d9fed9-3f40-461b-8ca3-b827ccef5e33\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-90d9fed9-3f40-461b-8ca3-b827ccef5e33')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-90d9fed9-3f40-461b-8ca3-b827ccef5e33 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 34000,\n  \"fields\": [\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1610313887375197,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5.0,\n          1.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26268,\n        \"samples\": [\n          \"Purchased as a gift pantry item for a friend\",\n          \"Interesting early Debussy breaking out of Massenet mode\",\n          \"Easy to swallow\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32882,\n        \"samples\": [\n          \"These diapers are great. I got them to have here in the house when my 2 year old grandson was visiting. My daughter-in-law has been a Pampers user for all of her children, so I was afraid when she saw these she would not want to use them. However, surprisingly she did try them and to my surprise actually liked them. These are just as absorbent and fit my grandbaby as well as Pampers, and with one exception are comparable to their cruisers cousins. The price! Given that she now has three children, all different ages and at different stages of their training, these Amazon Brand are a great way for her to still 'pamper' her baby while saving much needed cash. After using them exclusively for the weekend, she asked me for the link of where to get these (so she could use them when she got home). I ended up buying her a pack and shipping it to her so it was there when she got home. But I know she is sold on these now and will most likely change to the Amazon brand when she goes for her next order.\",\n          \"I am an educator so this free download seemed great. Understand that this program is teaching a ball and stick, double stroke writing method that is not natural for young writers. You can still use the nice features but ignore the demo. All letters start at the top, and follow a smooth pattern in one stroke. If I try to offer suggestions, my rating won't be listed. Good luck.\",\n          \"This is always a reliable physical gift during the holidays! My friends enjoy this and love the reindeer that is attached to the gift box!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"images\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1971,\n        \"samples\": [\n          \"[{'small_image_url': 'https://m.media-amazon.com/images/I/614Ut3pVNPL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/614Ut3pVNPL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/614Ut3pVNPL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/61DNEvABIBL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/61DNEvABIBL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/61DNEvABIBL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/61O89+f9BYL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/61O89+f9BYL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/61O89+f9BYL._SL1600_.jpg', 'attachment_type': 'IMAGE'}]\",\n          \"[{'small_image_url': 'https://m.media-amazon.com/images/I/61S3DWhaVoL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/61S3DWhaVoL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/61S3DWhaVoL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/61d2jTTM2UL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/61d2jTTM2UL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/61d2jTTM2UL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/61moeWe8XvL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/61moeWe8XvL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/61moeWe8XvL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/71JGzS-ngpL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/71JGzS-ngpL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/71JGzS-ngpL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/71iZbHi7ixL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/71iZbHi7ixL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/71iZbHi7ixL._SL1600_.jpg', 'attachment_type': 'IMAGE'}]\",\n          \"[{'small_image_url': 'https://images-na.ssl-images-amazon.com/images/I/81e0e06KxTL._SL256_.jpg', 'medium_image_url': 'https://images-na.ssl-images-amazon.com/images/I/81e0e06KxTL._SL800_.jpg', 'large_image_url': 'https://images-na.ssl-images-amazon.com/images/I/81e0e06KxTL.jpg', 'attachment_type': 'IMAGE'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"asin\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31010,\n        \"samples\": [\n          \"B0052RDJ5Y\",\n          \"B08F9X2GLZ\",\n          \"B00TETXODI\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"parent_asin\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30406,\n        \"samples\": [\n          \"B0C68LVZZK\",\n          \"B0BN2MFT5H\",\n          \"B0051HEFAS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6462,\n        \"samples\": [\n          \"AHFLLZLXIWRHZIX2JCSUVNTFWUCQ\",\n          \"AHKOWV3W3KI4SKHLO3DC6FOI7SYA\",\n          \"AHAE6GIVSMB4B74IZYBWH5SATIFQ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 102946696815,\n        \"min\": 938204489000,\n        \"max\": 1679182693930,\n        \"num_unique_values\": 33996,\n        \"samples\": [\n          1552857732717,\n          1586729193943,\n          1610320826522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"helpful_vote\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 0,\n        \"max\": 985,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          63,\n          60,\n          131\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"verified_purchase\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 34,\n        \"samples\": [\n          \"Home_and_Kitchen\",\n          \"Video_Games\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "execution_count": 66
    },
    {
      "cell_type": "markdown",
      "source": "# Data Cleaning and Transformation (Preprocessing)",
      "metadata": {
        "id": "uQlzTHBMWIUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We are dropping columns that are not interesting to us:\n\n\n\n*   asin: Product ID\n*   parent_asin: parentID of a product (different color / style etc. products have a parent)\n* user_id: Id of the reviewer\n* images: images that a user posts with the review\n\n\nThen we drop html tags in the text and title and strip leading and trailing whitespaces.\nReviews with empty text and title are removed\n",
      "metadata": {
        "id": "G1HojScQ-XFS"
      }
    },
    {
      "cell_type": "code",
      "source": "df.drop(columns=[\"images\", \"asin\", \"parent_asin\", \"user_id\"], inplace=True, errors=\"ignore\")\n\ndf[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n\ndf = df.dropna(subset=[\"text\", \"title\", \"rating\"])\ndf[\"text\"] = df[\"text\"].str.replace(r\"<.*?>\", \"\", regex=True)  # drop html tags\ndf[\"title\"] = df[\"title\"].str.replace(r\"<.*?>\", \"\", regex=True)\ndf[\"text\"] = df[\"text\"].str.strip()\ndf[\"title\"] = df[\"title\"].str.strip()\ndf = df[df[\"text\"].str.len() > 0]\ndf = df[df[\"title\"].str.len() > 0]\n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXphdJPy8x5d",
        "outputId": "6531795f-4b04-4b2c-d974-b40921ecfb33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "<ipython-input-67-7d9a25737a77>:6: SettingWithCopyWarning: \n,A value is trying to be set on a copy of a slice from a DataFrame.\n,Try using .loc[row_indexer,col_indexer] = value instead\n,\n,See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n,  df[\"text\"] = df[\"text\"].str.replace(r\"<.*?>\", \"\", regex=True)  # drop html tags\n,<ipython-input-67-7d9a25737a77>:7: SettingWithCopyWarning: \n,A value is trying to be set on a copy of a slice from a DataFrame.\n,Try using .loc[row_indexer,col_indexer] = value instead\n,\n,See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n,  df[\"title\"] = df[\"title\"].str.replace(r\"<.*?>\", \"\", regex=True)\n,<ipython-input-67-7d9a25737a77>:8: SettingWithCopyWarning: \n,A value is trying to be set on a copy of a slice from a DataFrame.\n,Try using .loc[row_indexer,col_indexer] = value instead\n,\n,See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n,  df[\"text\"] = df[\"text\"].str.strip()\n,<ipython-input-67-7d9a25737a77>:9: SettingWithCopyWarning: \n,A value is trying to be set on a copy of a slice from a DataFrame.\n,Try using .loc[row_indexer,col_indexer] = value instead\n,\n,See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n,  df[\"title\"] = df[\"title\"].str.strip()\n"
        }
      ],
      "execution_count": 67
    },
    {
      "cell_type": "markdown",
      "source": "# Looking for answers\n\n\nTo answer our questions we use different models.\n\nWe want to analyze:\n\n\n*   Sentiment\n*   Subjectivity\n*   Aspects\n*   Rating prediction\n\nFor this we use models from kaggle that may not yield SOTA results, but are managable to run for our on device analysis.",
      "metadata": {
        "id": "c87GsJVSWnU_"
      }
    },
    {
      "cell_type": "code",
      "source": "# helper methods\n\ndef get_device(disable_mps=False) -> str:\n    if torch.backends.mps.is_available() and not disable_mps:\n        return \"mps\"\n    elif torch.cuda.is_available():\n        return \"cuda\"\n    else:\n        return \"cpu\"\n\ndef print_gpu_memory() -> None:\n  if torch.cuda.is_available():\n    print(f\"memory summary: {torch.cuda.memory_summary(device='cuda')}\")\n    print(f\"gpu memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n    print(f\"gpu memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n    print(f\"gpu memory peak: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n    print(f\"gpu memory peak cached: {torch.cuda.max_memory_reserved() / 1e9:.2f} GB\")\n\n\n\n",
      "metadata": {
        "id": "FC2636IqeOT-",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "execution_count": 68
    },
    {
      "cell_type": "markdown",
      "source": "# Aspect Extraction \nAspect extraction identifies the most important topics or themes from a piece of text. For this, we use the YAKE (Yet Another Keyword Extraction) library\n## How Yake works?\nYAKE computes a set of statistical features for each word in the text. These features help determine the importance of a word in the context of the document.\n\nKey features include:\n\n* Term Frequency (TF): How often a word appears in the document.\n    * Word Position: Words that occur earlier in the document may have higher significance.\n    * Context Window: The co-occurrence of words in proximity to one another.\n    * Case Sensitivity: The use of uppercase letters can indicate named entities or emphasized words.\n* Normalization: The relative importance of words is adjusted based on document length.\n* Word Scoring: Each word is scored using the extracted features. Words with lower scores are considered more relevant as keywords. YAKE \n    balances local importance (specific to the document) with global importance (how common the word is in general).\n * Phrase Construction: Words are combined into multi-word phrases (n-grams) based on their scores and their proximity in the text.\n* Deduplication: Similar phrases are merged or de-duplicated using techniques like Levenshtein distance or cosine similarity to avoid   \n    redundancy.\n * Output: The final output is a ranked list of keywords or phrases, with lower scores indicating higher relevance.\n\n## Why Use YAKE?\n* Unsupervised: YAKE does not require pre-labeled training data, making it suitable for diverse datasets.,\n* Efficient: It is computationally lightweight, processing text quickly.\\n\",\n* Customizable: Parameters like deduplication and n-grams can be tuned for better results.\n        \n  ### Advantages\n  1. Works well on small datasets.\n  2. Does not rely on external resources like pre-trained models.\n  3. Effective at identifying contextually relevant keywords.\n        \n  ### Disadvantages:\n  1. Limited to statistical features, which may result in less semantic understanding.\n  2. Performance depends heavily on text preprocessing and parameter tuning.\n  3. Can sometimes include irrelevant keywords.\n\n## Why did we use Yake?\n* simplicity: We needed a quick, lightweight keyword extraction method without requiring pre-trained models.\n* Scalability: YAKE is fast, which is essential for large-scale datasets like Amazon reviews.\n* Adaptability: It allowed us to extract both single words and two-word phrases (n=2) to capture aspects more comprehensively (e.g., \n        \"battery life\", \"customer service\").\n* Relevance: The deduplication parameter (dedupLim=0.3) ensured that we avoided redundant or overly similar phrases, making our \n        results more meaningful.\n\n## Example\nGiven a review like \"The battery life of this phone is excellent. However, the camera quality is poor.\"\n\nYake might output keywords like:\n* battery life (high relevance due to its frequency and context)\n* camera quality (similarly high relevance)\n* excellent (important because it emphasizes sentiment)\n* poor (adds nuance to sentiment analysis)\n### Implementation:\n  Below, we demonstrate how YAKE is used alongside other pipelines for sentiment and subjectivity\n  analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import yake\nfrom langdetect import detect\nfrom transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\n\n\ntry:\n    device = get_device(disable_mps=False)\nexcept:\n    device = \"cpu\"\nprint(f\"using device: {device}\")\n\n\nsentiment_pipeline = pipeline(\"sentiment-analysis\", device=device,\n                             model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n                             model_kwargs={\"cache_dir\": weights_path})\n\nsubjectivity_pipeline = pipeline(task=\"text-classification\", model=\"cffl/bert-base-styleclassification-subjective-neutral\",\n                                top_k=None, device=device, model_kwargs={\"cache_dir\": weights_path})\n\nrating_tokenizer = AutoTokenizer.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\",\n                                                device=device, cache_dir=weights_path)\nrating_model = AutoModelForSequenceClassification.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\").to(device)\n\nkw_extractor = yake.KeywordExtractor(lan=\"en\", n=2, dedupLim=0.3, top=10, features=None)\n\n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STNhRz-Ie-kr",
        "outputId": "cbc2bbe4-f0af-4dd9-c217-494423452b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "using device: cuda\n,results_path: /content/datasets/results_n1000.csv\n"
        }
      ],
      "execution_count": 69
    },
    {
      "cell_type": "markdown",
      "source": "### Function for Processing Rows:\nThis function applies sentiment analysis, subjectivity scoring, keyword extraction (aspects), and rating prediction to each row of the dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def process_row(row):\n    batch_size = len(batch)\n    review = f\"{row['title']}: {row['text']}\"\n    review = review[:512]  # Truncate once, at the beginning\n    try:\n        sentiment = sentiment_pipeline(review)[0]\n        sentiment_label = sentiment[\"label\"]\n        sentiment_score = sentiment[\"score\"]\n    except Exception as e:\n        print(\"error get_sentiment:\", e)\n        sentiment_label, sentiment_score = None, None\n\n    try:\n        subjectivity_score = subjectivity_pipeline(review)[0][0][\"score\"]\n    except Exception as e:\n        print(\"error get_subjectivity:\", e)\n        subjectivity_score = None\n\n    try:\n        keywords = kw_extractor.extract_keywords(review)\n        aspects = [kw for kw, score in keywords if score < 0.3]\n    except Exception as e:\n        print(\"error get_aspects:\", e)\n        aspects = None\n    try:\n        inputs = rating_tokenizer(review, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = rating_model(**inputs)\n        predicted_class_idx = outputs.logits.argmax().item()\n        predicted_class = rating_model.config.id2label[predicted_class_idx]\n        rating = float(predicted_class[0])\n    except Exception as e:\n        print(\"error get_rating:\", e)\n        rating = None\n\n    try:\n        language = detect(review)\n    except Exception as e:\n        print(\"error get_language:\", e)\n        language = None\n\n    return {\n        \"language\": language,\n        \"sentiment\": sentiment_label,\n        \"sentiment_score\": sentiment_score,\n        \"subjectivity_score\": subjectivity_score,\n        \"aspects\": aspects,\n        \"rating\": rating,\n    }\n\nresults_path = dataset_path / f\"results_n{sample_size}.csv\"\nprint(f\"results_path: {results_path}\")\n\nif not results_path.exists():\n    tqdm.pandas()\n    results = df.progress_apply(process_row, axis=1)\n    results.to_csv(results_path, index=False)\nelse:\n    results = pd.read_csv(results_path)\n#only works on reload from csv?\nresults = results[\"0\"].apply(lambda x: pd.Series(eval(x)))\nresults = results.rename(columns={\"rating\": \"predicted_rating\"})\ndf_results = pd.concat([df, results], axis=1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_results.head()\ndf_results.to_csv(data_path / \"data.csv\", index=False)\nprint(\"saved data\")",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld2qpd7LckXT",
        "outputId": "64907672-9dd3-47e2-b946-9011b58c6be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "saved data\n"
        }
      ],
      "execution_count": 71
    },
    {
      "cell_type": "markdown",
      "source": "# What did we find",
      "metadata": {
        "id": "Cc_9UNJHWzFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Data quality and difficulties",
      "metadata": {
        "id": "FGuQijqxWp9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# What did we learn",
      "metadata": {
        "id": "A1CK9C2uW2SH"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Who did what",
      "metadata": {
        "id": "x24To2HXW4V6"
      }
    }
  ]
}