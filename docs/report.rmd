---
title: "Report: Amazon Reviews'23"
subtitle: "Code: [`github.com/sueszli/amazeballs/`](https://github.com/sueszli/amazeballs/)"
output: pdf_document
documentclass: article
papersize: a4
pagestyle: empty
geometry:
    - top=5mm
    - bottom=5mm
    - left=5mm
    - right=5mm
header-includes:
    # title
    - \usepackage{titling}
    - \setlength{\droptitle}{-15pt}
    - \pretitle{\vspace{-30pt}\begin{center}\LARGE}
    - \posttitle{\end{center}\vspace{-50pt}}    
    # content
    - \usepackage{scrextend}
    - \changefontsizes{8pt}
    # code
    - \usepackage{fancyvrb}
    - \fvset{fontsize=\fontsize{6pt}{6pt}\selectfont}
    - \usepackage{listings}
    - \lstset{basicstyle=\fontsize{6pt}{6pt}\selectfont\ttfamily}
    # code output
    - \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\fontsize{6pt}{6pt}}
---

<!-- 

https://tuwel.tuwien.ac.at/pluginfile.php/4247741/mod_resource/content/1/DOPP2024_Exercise2.pdf

this is just as much about the "data science process" as it is about the results.

this is an open ended task.

we're free to pick whatever dataset we like and modify questions with supervisor's approval.

deliverables:

- plan / review meeting document (1 page)
        - research questions
        - datasets planned to use
        - methodology to answer questions
        - division of work
- report (2 pages)
        - management summary document
        - main insights
- a single jupyter notebook
        - like a more verbose version of the report
- presentation (10min)

-->

#### Motivation

Understanding online reviews isn't just about interpreting customer opinions; it's a window into how people perceive and interact with products across diverse categories. The Amazon Dataset'23 offers a treasure trove of insights, allowing us to explore patterns in sentiment, subjectivity and the elements that matter most in consumer decision-making. By digging into this data, we aim to uncover the subtle relationships between what customers say and how they rate products, shedding light on the dynamics of trust, satisfaction and expectation in the digital marketplace.

Beyond the findings, this report highlights the (data science) process of turning raw, unstructured data into actionable knowledge. Through techniques like sentiment analysis, topic modeling and classification, we're not just addressing key questions about product reviews â€“ we're also demonstrating the iterative, hands-on nature of data science itself.

#### Process

The process which we followed, is more formally known as CRISP-DM (Cross-Industry Standard Process for Data Mining). It begins with (1) business understanding, where we refine the research questions in consultation with a supervisor for our project, define variables and metrics and build hypotheses while being mindful of biases. Next, we move to (2) data understanding, where we sample and preprocess the data, ensuring privacy and assess the accuracy, biases and reliability of the measurements. In (3) data preparation, we clean the data by checking for missing values, outliers and inconsistencies, calculating descriptive statistics and transforming the data as needed. If the data is insufficient to answer the research questions, we may combine columns, look for additional datasets, or modify the questions. In (4) modeling, we calculate correlations and build models to explore the relationships between variables. During (5) evaluation, we plot the data, identify patterns and anomalies, visualize the findings and check predictions to assess if the models answer the original questions. Finally, (6) deployment involves using the results to make decisions or share insights with stakeholders.

# Methodology

<!-- What questions did you ask of the data? Why were these good questions? -->

#### Research Questions

First we define the research questions that we aim to answer. Our team has selected task 21 from the list provided by the course team and did not further modify them, as they already cover a wide range of topics for customer review analysis. The research questions are as follows:

- (RQ1) Are reviews for some categories of product on Amazon overall more positive than for other categories?
- (RQ2) Are reviews more subjective for some classes of products than for others?
- (RQ3) Which aspects of different classes of products are the most important in the reviews?
- (RQ4) Can one predict the star rating from the review text?

The first research question is a comparison of sentiment across categories, the second is a comparison of subjectivity across categories, the third is a topic modeling task, commonly referred to as aspect-based sentiment analysis and the fourth is a classification task to predict the star rating from the review text.

For the sentiment analysis task (RQ1) we used a pre-trained and distilled version of a multi-lingual Bidirectional Encoder Representations from Transformers (BERT) model to classify the sentiment of the reviews into positive, negative or neutral classes in addtion to a sentiment score. We were able to notice that lanauges other than English were also present in the dataset by using the `langdetect` library, however, not all following models were multi-lingual and we thus had to tolerate some errors, especially in the aspect extraction task.

Subjectivity (RQ2) was again determined using a BERT model. But this time it was tuned on the "Wiki Neutrality Corpus dataset" which indirectly adopts Wikipedia's NPOV policy as the definition for "neutrality" and "subjectivity". The NPOV policy may not fully reflect an end users assumed or intended meaning of subjectivity because ironically enough, the policy itself is subjective. However, it is a good starting point for a model to learn what is considered neutral and what is not and suitable for our small scale project.

The aspect extraction task (RQ3) was done using an inaccurate but highly efficient keyword extraction algorithm YAKE! which is based on the TextRank algorithm. Due to compute limitations, we were not able to use a more accurate models like SetFitABSA [^absa] or `pyabsa` for aspect extraction. However we did implement them in case the reader is interested in running them on their own machine.

Finally, for the star rating prediction task (RQ4) we used a pre-trained BERT model fine-tuned on an older and exclusively English version of the Amazon Reviews dataset, reaching an accuracy of 0.8. This model was able to predict the star rating of a review with a high degree of accuracy.

[^absa]: Jayakody, D., Isuranda, K., Malkith, A. V. A., De Silva, N., Ponnamperuma, S. R., Sandamali, G. G. N., & Sudheera, K. L. K. (2024, August). Aspect-based Sentiment Analysis Techniques: A Comparative Study. In 2024 Moratuwa Engineering Research Conference (MERCon) (pp. 205-210). IEEE.

<!-- Which dataset(s) did you choose? Why? -->

#### Dataset Selection

To answer these questions, we chose the [Amazon Reviews'23 dataset](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023) which is the standard dataset for the Amazon product reviews in the RecSys and NLP communities. This dataset is a collection of 571.54M reviews, 245.2% larger than the last version, with interactions ranging from May 1996 to September 2023. It includes richer metadata, fine-grained timestamps and cleaner processing, making it an ideal choice for our analysis. Most importantly it is easily accessible through the Hugging Face Datasets library, which simplifies the data loading and preprocessing steps.

<!-- How did you clean/transform the data? Why? -->

#### Data Preprocessing

The data cleaning and transformation process involved several key steps to manage the dataset's size and ensure it was suitable for analysis. Initially, the dataset contained 100,000 samples per category, which amounted to 2.92 GB and was too large to fit in memory for plotting. This necessitated reducing the dataset size to make it manageable.  First, we considered sampling 10,000 samples per category, which would reduce the dataset size to 0.33 GB. However, this would have resulted in an inference time of 8 days due to the large number of items (339,880) and the slow processing speed of 2 items per second. Subsequently, we explored sampling 1,000 samples per category, which would further reduce the dataset size to 0.03 GB. However, this would still require 19 hours for inference, which was deemed too long for practical use. Finally, we decided to sample only 100 samples per category, resulting in a dataset size of less than 0.00 GB. This smaller dataset contained 3,399 items, which could be processed within a reasonable timeframe of approximately 2 hours at a rate of 2 items per second. This approach allowed for effective model inference and analysis while maintaining a manageable dataset size and was small enough to push to git.

The data transformation process involved (1) loading the data from HuggingFace and intermediate caching on multiple levels to avoid redundant bandwidth usage, (2) sampling the data to reduce the dataset size using a fixed seed for reproducibility, (3) preprocessing the data by dropping unnecessary columns, converting timestamps to datetime objects, removing rows with missing values in critical columns, and cleaning text fields by stripping HTML tags and whitespace, and filtering out empty strings. Finally, the cleaned data was prepared for inference tasks and the subsequent analysis.

<!-- How did you solve the problem of missing values? Why? -->

#### Missing Values

Missing values in the dataset are addressed through a combination of data preprocessing techniques. The primary method used is listwise deletion, where rows with missing values in specific columns are removed. This is evident in the preprocess function, where the `dropna` method is applied to remove rows with missing values in the "text", "title", and "rating" columns. This approach ensures that only complete cases are included in subsequent analyses, which can help maintain the integrity of the dataset but may lead to a loss of data if many entries are incomplete. Additionally, the code handles missing values by cleaning text data to ensure that any remaining entries are valid. For instance, HTML tags are stripped from the "text" and "title" fields, and whitespace is removed. This step ensures that any non-informative or empty text entries are filtered out, thereby improving data quality.

The choice of listwise deletion is appropriate when missing data is assumed to be missing completely at random (MCAR), as it can prevent bias in analysis if this assumption holds true. However, if the data is not MCAR, this method might introduce bias and reduce statistical power. Alternative methods such as imputation (mean, median, mode) or more sophisticated techniques like multiple imputation could be considered if preserving all available data is critical and if the missingness pattern allows for such methods.

<!-- What are potential biases in the data and analysis? -->

#### Potential Biases

Firstly, there is a significant sampling bias due to the nature of the data collection process, which relies on reviews from Amazon. This platform-specific focus can lead to a lack of representativeness, as it excludes opinions from other e-commerce platforms and may reflect the purchasing habits and preferences unique to Amazon users. Additionally, many reviews are from users who have only left a single review, which suggests a reporting bias where occasional reviewers might not provide as balanced or informed feedback as more frequent reviewers.

The data processing phase introduces further biases. For instance, reviews are truncated to 512 tokens for analysis, potentially omitting important context or nuances found in longer reviews. Moreover, the language detection and sentiment analysis models used are primarily trained on English data, which could introduce language bias if reviews in other languages are not accurately processed or interpreted.

The models employed for sentiment analysis and aspect extraction also carry intrinsic biases. These models are trained on specific datasets that may not fully capture the diversity of consumer opinions or product types found on Amazon. For example, sentiment analysis models might oversimplify complex sentiments into basic categories like positive or negative, thus failing to capture more nuanced consumer feedback.

Additionally, there is a notable presence of fake, paid, or incentivized reviews on Amazon, which can significantly skew the perceived quality and satisfaction levels of products. These practices compromise the reliability of reviews and can mislead consumers by presenting an inaccurate picture of product quality. The combination of these biases suggests that while Amazon reviews can provide valuable insights, they should be interpreted with caution and supplemented with more objective sources when possible.

<!-- Which Data Science tools and techniques were learned during this exercise? -->
<!-- Were there any difficulties in analysing the data? -->

#### Challenges & Lessons Learned

This project involved a range of data science tools and techniques. From a technical perspective, we used a reproducible `virtualenv` environment, a `venv` to `docker` and `conda` transpiler written in a makefile, to ensure both portability and reproducibility through the compilation of a requirements text file. Additionally, we used the `datasets` library to load and preprocess the dataset. This is standard practice. Using these technologies we learned how to handle the challenges of working on data in teams, and managing large datasets with very limited computational resources.

From a theoretical perspective We also explored aspect extraction using keyword extraction and aspect-based sentiment analysis. The project provided hands-on experience with real-world data analysis, including data preprocessing, model training and evaluation. The team also gained insights into the challenges of working with large datasets, multilingual reviews and the importance of sampling and model selection in data analysis.

<!-- How was the work divided up between the members of the group? -->

#### Team Work Division

Tasks were divided based on expertise: data preprocessing, sentiment analysis, aspect extraction and sampling were handled by different team members. Each person focused on the components that best suited their skills. We exclusively worked in-person and collaborated through pair-programming sessions.

# Results

<!-- What were the answers to these questions? How did you obtain them? Do the answers make sense? -->

<!-- What were the key insights obtained? -->

```{r, echo=FALSE, message=FALSE, warning=FALSE}
deps <- c("ISLR", "Metrics", "tidyr", "microbenchmark", "dplyr", "gridExtra", "readxl", "cvTools", "leaps", "assertthat", "assertr", "testthat", "lubridate", "ggplot2", "stringr", "scales", "viridis")
for (p in deps) {
    if (!requireNamespace(p, quietly = TRUE)) {
        install.packages(p, repos = "https://cran.rstudio.com")
    }
    library(p, character.only = TRUE)
}

options(scipen=999)
set.seed(42)

df <- read.csv("../data/data.csv", header = TRUE, sep = ",")

df$category[is.na(df$category)] <- "unknown"
```

## RQ1: Sentiment Analysis

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=15, fig.height=8}
median_sentiment <- aggregate(df$sentiment_score, by = list(Category = df$category), FUN = median)
ordered_categories <- median_sentiment$Category[order(median_sentiment$x)]

p1 <- ggplot(df, aes(x = factor(category, levels = ordered_categories), y = rating)) +
    geom_count(aes(size = after_stat(prop), group = 1)) +
    scale_size_area(max_size = 7) +
    coord_flip() +
    labs(title = "Rating Distribution by Product Category", x = "Category", y = "Rating") +
    theme(axis.text.y = element_text(size = 8)) +
    theme_minimal()

global_median <- median(df$sentiment_score, na.rm = TRUE)

p2 <- ggplot(median_sentiment, aes(x = factor(Category, levels = ordered_categories), y = x)) + 
    geom_bar(stat = 'identity', fill = 'gray') + 
    geom_hline(yintercept = global_median, color = "red", linetype = "dashed", size = 1, alpha = 0.5) +
    annotate("text", x = 1, y = global_median, label = "Global Median", vjust = -1, hjust = 1.1, color = "red") +
    coord_flip() +
    labs(title = 'Median Sentiment Score by Product Category', x = 'Category', y = 'Median Sentiment Score') +       
    theme(axis.text.y = element_text(size = 8)) +
    theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

## RQ2: Subjectivity Analysis

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=15, fig.height=8}
p1 <- ggplot(df, aes(x = reorder(category, subjectivity_score, median), y = subjectivity_score)) +
    geom_boxplot(fill = "lightgray") +
    coord_flip() +
    theme_minimal() +
    labs(title = "Subjectivity Score Distribution by Product Category", x = "Category", y = "Subjectivity Score") +
    theme(axis.text.y = element_text(size = 8))

median_subjectivity <- aggregate(df$subjectivity_score, by = list(Category = df$category), FUN = median)
ordered_categories_subjectivity <- median_subjectivity$Category[order(median_subjectivity$x)]
global_median_subjectivity <- median(df$subjectivity_score, na.rm = TRUE)

p2 <- ggplot(median_subjectivity, aes(x = factor(Category, levels = ordered_categories_subjectivity), y = x)) + 
    geom_bar(stat = 'identity', fill = 'gray') + 
    geom_hline(yintercept = global_median_subjectivity, color = "red", linetype = "dashed", size = 1, alpha = 0.5) +
    annotate("text", x = 1, y = global_median_subjectivity, label = "Global Median", vjust = -1, hjust = 1.1, color = "red") +
    coord_flip() +
    labs(title = 'Median Subjectivity Score by Product Category', x = 'Category', y = 'Median Subjectivity Score') +       
    theme(axis.text.y = element_text(size = 8)) +
    theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

## RQ3: Aspect Extraction

## RQ4: Star Rating Prediction

